# ImageToVec encoder for proximity search.

## Reference papers ðŸ“ƒ:
- [SimCLR. A Simple Framework for Contrastive Learning of Visual Representations.](https://arxiv.org/abs/2002.05709)
- [Clustering-based Contrastive Learning for Improving Face Representations](https://arxiv.org/pdf/2004.02195)
- [Locality-Sensitive Hashing for Finding Nearest Neighbors](https://ieeexplore.ieee.org/abstract/document/4472264)
- ...

## Topic 

### *Computer Vision* ðŸ¤–

Image encoding to vector space.


## Project type 

### *Hybrid*
Combination of **Bring your data** and **Bring your method**.

## Project description

### Idea ðŸ–¼ï¸
The core idea is to encode the image to a vector space for future proximity search and clustering.

In particular, I am interested in painting. Imagine painting something and then finding out which style, years, school, or authors it is most similar to.

### Model and Techniques ðŸ”§
The current plan is to train/fine-tune CNN with Contrast learning techniques to create an Image encoder. Later, some locality approximation algorithms will be used to optimize search and clustering.

### Data ðŸ’¾
Data will be collected (scrapped) from the [Web Gallery of Art](https://www.wga.hu/index.html), which has approximately 50,000 art pieces and 6,000 authors. Of course, some additional sources might be considered.

## Plan of Work ðŸ“…

- Additional research on the topic (~4 hours)
- Data collection scripts and verification  (~5 hours)
- Small model creation (~3? hours)
- Small model testing (~30 min)
- Pipeline creation (~1 hours)
- Model training ( ?0 hours )
- Proof of concept testing (~30 min)
- Web dashboard interface (~4 hours)


# Phase 1 Results

Progress :

- Data Scraping âœ…
- Images Scraping âœ…
- Data Preprocessing âœ…
- Train Indexes Pre-computation âœ…
- Pipeline base model âœ…
- Basic embedding modeling âœ…
- Base tests and Approximation algorithms for search âŒ
- Full model train and test âŒ
- UI application âŒ

## Details 

### Data Scraping

Deliverables:

>/data (Retrieved data)

>/data_mining (Notebooks)

As mentioned in task description, the data is scraped from [Web Gallery of Art](https://www.wga.hu/index.html)

DataScraping.ipynb Is collecting the Artist (style, name, period ... ) data and combines it with art pieces data(img_catalog.txt) collected from website. Additionally basic data transformation is done.

### Data Scraping

Deliverables:

>/image_data (Retrieved data)

>/data_mining (Notebooks)

DataScraping.ipynb Is downloading images. Broken links are removed.


### Train Indexes Pre-computation

Deliverables:

>/indexes_data (Retrieved data)

>/Create_full_test_set.ipynb
>/Create_mini_test_set.ipynb

Full data set considers:

Each image has school type and style, as characteristics. For training purposes each image (anchor) contains a set of all Ideal positive examples and all set of Ideal negative examples. Ideal means (**full** characteristics match or not match). 

For efficiency the data split into groups(f.e. French - Impressionism - Landscape ) as all of this group will have the same negative and positive examples.

The mini data (used for evaluation and as a test) is considers only one characteristic (style (as the most difficult)). Everything is the same.


### Pipeline base model

>/model (Trained model)

>/BaseLine_pipe.ipynb 


The basic model is can be represented as follows:

![piplene](readme_images\pipline.png)

Where the x1 and x2 are images. (Anchor and positive or negative example)

f0 is the CNN model.

Features are Fully connected layers, that are being trained.

Comparison is Contrastive loss function.

![piplene](readme_images\formula.png)


For the data, custom dataset and dataloader are created.

For the base model, the pre-trained 

>ShuffleNet_V2_X1

was used as lightweight CNN layers for feature extraction.

### Model modification

For the training purposed the CNN layers are frozen and the last clarification layers are replaced with dense layers.


### Basic embedding modeling

>/Show_Embeddings.ipynb

>/space.png

The notebook is using the trained baseline model for creating basic embeddings for small subset of mini_test_set.

The embedding are later projected to 2d space with UMAP and displayed on graph.


![space](readme_images\space.png)


One can observe that Model is creating some more-less meaningful clusters.  
